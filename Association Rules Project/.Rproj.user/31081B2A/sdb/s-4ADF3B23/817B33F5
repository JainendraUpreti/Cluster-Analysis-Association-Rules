{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Data Mining BANA - 7046(002) Homework-4\"\nauthor: \"Gupta, Akash / Joshi, Apoorv /  Thapar, Sahil / Upreti, Jainendra\"\ndate: \"14 February, 2017\"\noutput:\n  html_document: default\n  pdf_document: default\n---\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = FALSE)\nknitr::opts_chunk$set(cache = TRUE)\nknitr::opts_chunk$set(message = FALSE)\nknitr::opts_chunk$set(warning = FALSE)\n```\n#Solution 1\n#i) EDA and Linear Regression\nThe Boston Housing Data is sourced from the UCI Machine Learning Repository [https://archive.ics.uci.edu/ml/datasets/Housing](https://archive.ics.uci.edu/ml/datasets/Housing).\nThe data set is available for free download and records the housing values in Boston suburban areas. The data measures 14 attributes across 506 instances.The Boston Housing Data is sourced from the UCI Machine Learning Repository.  \n\n## Data description\n\n1. crim - per capita crime rate by town.\n2. zn - proportion of residential land zoned for lots over 25,000 sq.ft\n3. indus - proportion of non-retail business acres per town\n4. chas - Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n5. nox - nitrogen oxides concentration (parts per 10 million)\n6. rm - average number of rooms per dwelling.\n7. age - proportion of owner-occupied units built prior to 1940\n8. dis - weighted mean of distances to five Boston employment centres\n9. rad - index of accessibility to radial highways.\n10. tax - full-value property-tax rate per \\$10,000\n11. ptratio - pupil-teacher ratio by town\n12. black - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.\n13. lstat - lower status of the population (percent).\n14. medv - median value of owner-occupied homes in \\$1000s.\n```{r}\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(tibble)\nlibrary(ggplot2)\nlibrary(MASS)\nlibrary(scales)\nlibrary(tibble)\nlibrary(Hmisc)\nlibrary(corrplot)\nlibrary(leaps)\nlibrary(boot)\nlibrary(rpart)\n# Sampling the data\n\nset.seed(999)\nsubset = sample(nrow(Boston), nrow(Boston) * 0.7)\n\nboston_sample = Boston[subset, ]\nboston_test = Boston[-subset, ]\n```\n##Histograms\n```{r, fig.cap = \"Histograms\"}\nboston_sample %>% \n  dplyr::select(c(nox, rm, age, dis, crim, medv)) %>%\n  gather(key = \"attribute\", value = \"value\", 1:6) %>%\n  ggplot() +\n  geom_histogram(mapping = aes(x = value), bins=10,\n                 fill = \"#2980b9\") +\n  facet_wrap(~attribute, scales = \"free\")\n```\n* age - The distribution for the age of the house appears to be left skewed.  \n* crim - The crime rate distribution across Boston appears to be usually low and hence a right skewed distribution can be observed.  \n* dis - The distance from the main metropolitan locations, similar to crime rate is usually less but has a few outliers.  \n* nox - The nitrogen oxide concentrations distribution appears to be multi modal with peaks and valleys.  \n* medv and rm - Both the median value of the house and the number of rooms in the house appear to be normally distributed.  \n\n##Boxplots\n```{r, fig.cap = \"Box plots\", fig.height = 10, fig.width = 8}\nboston_sample %>% \n  dplyr::select(-c(chas)) %>%\n  gather(key = \"attribute\", value = \"value\", 1:13) %>%\n  ggplot() +\n  geom_boxplot(mapping = aes(x = attribute,\n                             y = value),\n               fill = \"#1abc9c\") +\n  facet_wrap(~attribute, scales = \"free\")\n```\nWe see a large number of outliers in __black population, crime rate, land zone, rooms and meain value__. Outliers are also seen in __dis, lstat, ptratio__\n\n##Correlation matrix\n```{r, fig.cap = \"Pairwise correlation\", fig.width = 9, fig.height = 9}\nboston_cor <- cor(boston_sample)\ncorrplot(boston_cor, method = \"square\")\n```\nInterpreting the above chart, \n__a dark blue square__ indicates a strong positive correlation while __a dark red square__ indicates a strong negative relation.\n\nThe strongest correlations are found in the following cases:  \nPositive:  \n* crime and rad - Areas with higher access to radial highways also have high crime rate  \n* industrial area and nitrogen oxide - indicates high nitrogen oxide also have more industrial areas.  \n* industrial areas and propterty tax - indicating industrial areas also have higher property tax rates  \n* industrial areas and age - More industrial areas also have higher concentration of older homes  \n* industrial areas and lower status population - A higher percentage of the lower status population lives near the industrail areas.  \n* nox and rad - The higher high nitogen oxide concentration areas are farther away from radia highways.  \n* rad and tax - property tax near radial highways are lower  \n* medv and rm - Higher valued homes have more rooms  \n\nNegative:  \n* indus and dis - industrial areas are located farther from the central locations  \n* nox and dis - areas with higher nitrogen oxide concentration are located farther from central locations  \n* age and dis - older homes are farther away from the central locations  \n* lstat and medv - the lower status concentrated areas have lower median house prices.  \n\n##Linear Regression\nLet us create a linear model to estimate the median value price of a house based on all other attributes.\n```{r}\n#Full Model\nbos.reg <- lm(medv ~., data=boston_sample)\nsum.bos <- summary(bos.reg)\nsum.bos\n```\n* __MSE__: `r round((sum.bos$sigma)^2, 2)`\n* __$R^2$__ : `r round(sum.bos$r.squared, 2)`\n* __Adjusted $R^2$__ : `r round(sum.bos$adj.r.squared, 2)`\n* __AIC__: `r round(AIC(bos.reg), 2)`\n* __BIC__: `r round(BIC(bos.reg), 2)`\n\nWe see that p-value for the model is $< 2.2e^{-16}$ and the model is significant.\nThe significant attributes are __zn, nox, rm, dis, rad, tax, prratio, black, lstat__\n\n##Residual Analysis\nLet's conduct residual analysis for the full model.\n\n```{r, fig.height=3.5, fig.width=3.5}\nplot(bos.reg)\n```\n\n__Residual vs Fitted__\n\nA slight underlying pattern can be observed which indicates there might be better (non-linear models) that are more capable of fitting the data.\n\n__Q-Q Plot__\n\nThe underlying data doesn't appear to be normal as we can see a tail clearly.\n\n__Scale-location__\n\nUnderlying pattern is observable indicating possible heteroscedasticity.\n\n__Residuals vs Leverage__\n\nWe can see that no points are seen across the red lines and our model is not affected by outliers much.\n\n## Variable selection\n\n### Best subset regression\nWe will try to find the best variables for predicting the house prices in Boston using the `regsubsets` function from the `leaps` package.\n\n```{r, fig.height = 6, fig.width = 6, fig.cap = \"Subset regression\"}\nsubset_result = regsubsets(medv ~ ., data = boston_sample, nbest = 2, nvmax = 14)\nplot(subset_result, scale = \"bic\")\n```\nThe above plot shows the different variables selected for different model 'complexity'. A black square indicates selection of variable while an empty space indicates that the variable was not selected in the model.\n\nInterpretations: \n __lstat, rm and ptratio__ are the best predictors and are selected in almost all regression subsets. __dis, nox, chas, black__ are the variables that can perhaps be included to get a more accurate model by compromising the model complexity.\n\n#ii) Testing in-sample and out-of-sample performance\nLet's consider the 3 variable model, with lstat, rm and ptratio as the predictors. These predictors are used to create the linear model and the test dataset is then used to validate the created model.\n```{r}\nlin_reg <- lm(medv ~ lstat + rm + ptratio, data=boston_sample)\nsumm_lm <- summary(lin_reg)\nsumm_lm\n```\n* __MSE__: `r round((summ_lm$sigma)^2, 2)`\n* __$R^2$__ : `r round(summ_lm$r.squared, 2)`\n* __Adjusted $R^2$__ : `r round(summ_lm$adj.r.squared, 2)`\n* __AIC__: `r round(AIC(lin_reg), 2)`\n* __BIC__: `r round(BIC(lin_reg), 2)`\n\n```{r}\npred <- predict(lin_reg, boston_test)\n```\nThe predict() function is used in R to calculate validate the model on the boston test dataset.  \nThe out-of-sample error MSE is calculated as: __`r round(mean((pred - boston_test$medv)^2),2)`__\n\n#iii) Cross Validation\n__cv.glm()__ function is used to perform 3-fold Cross Validation on the original Boston dataset. \n```{r}\nmodel_3 = glm(medv ~ ., data = Boston)\ncv.error<-cv.glm(data = Boston, glmfit = model_3, K = 3)$delta[2]\nsummary(model_3)\n```\nMSE obtained is: __`r round(cv.error, 2)`__\nThe MSE calculated is comparable to the in-sample MSE obtained for the full model in part i). However, it is subtantially lower than the MSE for the reduced 3-variable model created in ii) \n\n#iv) and v) CART (Regression Tree)\nRegression tree is created from the boston training subset. The boston test subset is then used to validate the model We use the __rpart__ function to fit the regression tree:\n\n```{r, fig.height=7, fig.width=7}\n\nboston.rpart <- rpart(formula = medv ~ ., data = boston_sample)\nplot(boston.rpart)\ntext(boston.rpart)\n\npred_insample = predict(boston.rpart)\npred_outsample = predict(boston.rpart, boston_test)\n```\nThe above figure indicates that the data is split into 8 parts. The most important predictor in the model is found to be 'rm' which is the number of rooms in the house. This means that more the number of rooms in the house, higher the chance that the house will be expensive. The tree also indicates the inverse relationship between _lstat_ which is the 'lower status of the population'. Thus, greater the value of lstat, the cheaper the dwelling.  \nThe in-sample error is calculated as __`r round(mean((pred_insample - boston_sample$medv)^2),2)`__ and the out-of-sample error is calculated as: __`r round(mean((pred_outsample - boston_test$medv)^2),2)`__\nIt can be seen that the calculated error is much lesser than the MSE calculated in case of linear regression.\n\n\n# vi) Regression Tree with a new sample\nA Boston dataset is again split into training and test datasets. However, to make sure that we have a different subset in this iteration we change the value held by the set.seed function. The training dataset, as earlier, contains 70% of the data points and the test dataset contains 30%. We create the regression tree using the new training dataset and then run it with the test dataset. The new tree has been displayed below:\n```{r fig.height=7, fig.width=7}\nset.seed(100)\nsubset1 = sample(nrow(Boston), nrow(Boston) * 0.7)\nboston_sample1 = Boston[subset1, ]\nboston_test1 = Boston[-subset1, ]\n\nboston.rpart1 <- rpart(formula = medv ~ ., data = boston_sample1)\n\nplot(boston.rpart1)\ntext(boston.rpart1)\n\npred_insample1 = predict(boston.rpart1)\npred_outsample1 = predict(boston.rpart1, boston_test1)\n```\n\n# GAM code\n```{r}\nlibrary(mgcv)\ngam_formula = as.formula(paste(\"medv~s(crim)+s(zn)+\n                                   s(nox)+s(indus)+s(rm)+s(dis)+s(ptratio)+s(black)+s(lstat)+\",\n                                   paste(colnames(boston_sample)[1:13], collapse = \"+\")))\n    Boston.gam = gam(gam_formula,method=\"REML\", data=boston_sample)\n    summary(Boston.gam)\n    plot(Boston.gam, shade = TRUE, , seWithMean = TRUE, scale = 0)\n    AIC(Boston.gam)\n    BIC(Boston.gam)\n    Boston.gam$deviance\n\n# Prediction using GAM####\n    #in-sample prediction\n    boston.train.pred.gam = predict(Boston.gam, boston_sample)\n    mean((boston.train.pred.gam - boston_sample$medv)^2)\n    \n    # Out of sample Prediction\n    boston.test.pred.gam = predict(Boston.gam, boston_test)\n    mean((boston.test.pred.gam - boston_test$medv)^2)\n```\n\n# Neural Net\n```{r}\n# Neural Network####\n    library(nnet)\n    Boston.nnet <- nnet(medv ~ ., size = 4, data = boston_sample, linout = TRUE)\n    \n    # Prediction using Neural Network####\n    #In-sample Prediction\n    boston.train.pred.nnet = predict(Boston.nnet, boston_sample)\n    mean((boston.train.pred.nnet - boston_sample$medv)^2)\n    \n    # Out of sample Prediction\n    boston.test.pred.nnet = predict(Boston.nnet, boston_test)\n    mean((boston.test.pred.nnet - boston_test$medv)^2)\n\n    #Neural Network Scaling #####\n    data1=MASS::Boston\n    maxs <- apply(data1, 2, max) \n    mins <- apply(data1, 2, min)\n    \n    scaled <- as.data.frame(scale(data1, center = mins, scale = maxs - mins))\n    summary(scaled)\n    \n    train_ <- scaled[subset,]\n    test_ <- scaled[-subset,]\n    \n    nrow(train_)\n    \n    library(neuralnet)\n    n <- names(train_)\n    f <- as.formula(paste(\"medv ~\", paste(n[!n %in% \"medv\"], collapse = \" + \")))\n    nn <- neuralnet(f,data=train_,hidden=c(5,3),linear.output=T)\n    plot(nn)\n    \n    pr.nn <- compute(nn,train_[,1:13])\n    \n    pr.nn_ <- pr.nn$net.result*(max(data1$medv)-min(data1$medv))+min(data1$medv)\n    train.r <- (train_$medv)*(max(data1$medv)-min(data1$medv))+min(data1$medv)\n    \n    MSE.nn <- sum((train.r - pr.nn_)^2)/nrow(train_)\n    MSE.nn\n    \n    \n    pr.nntest <- compute(nn,test_[,1:13])\n    \n    pr.nn_test <- pr.nntest$net.result*(max(data1$medv)-min(data1$medv))+min(data1$medv)\n    \n    test.r <- (test_$medv)*(max(data1$medv)-min(data1$medv))+min(data1$medv)\n    \n    MSE.nn_test <- sum((test.r - pr.nn_test)^2)/nrow(test_)\n    MSE.nn_test\n```\n\nIn the new regression tree, variable lstat has been judged as the most important followed by rm. However, we still have 8 nodes. Again, this data indicates that lstat is inversely proportional and rm is directly proportional to the value of the house.\nThe in-sample error is calculated as __`r round(mean((pred_insample1 - boston_sample1$medv)^2),2)`__ and the out-of-sample error is calculated as: __`r round(mean((pred_outsample1 - boston_test1$medv)^2),2)`__\nThe above values are different from the ones we obtained in iv) and v) since we used a different sample of the dataset to create our model.\n",
    "created" : 1497988958485.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "915324254",
    "id" : "817B33F5",
    "lastKnownWriteTime" : 1490073029,
    "last_content_update" : 1490073029,
    "path" : "J:/Data mining/Assignment4.Rmd",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}